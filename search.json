[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EarlyDarkEmu",
    "section": "",
    "text": "emulator for P(k) for dark matter power spectra",
    "crumbs": [
      "EarlyDarkEmu"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "EarlyDarkEmu",
    "section": "Install",
    "text": "Install\n(not installable yet)\npip install EarlyDarkEmu\n(Use this development version instead)\ngit clone https://github.com/nesar/EarlyDarkEmu.git\ncd EarlyDarkEmu/\npip install -e '.[dev]'",
    "crumbs": [
      "EarlyDarkEmu"
    ]
  },
  {
    "objectID": "index.html#basic-rundown",
    "href": "index.html#basic-rundown",
    "title": "EarlyDarkEmu",
    "section": "Basic rundown",
    "text": "Basic rundown\n\nA few imports\n\nfrom EarlyDarkEmu.load import *\nfrom EarlyDarkEmu.viz import *\nfrom EarlyDarkEmu.pca import *\nfrom EarlyDarkEmu.gp import *\nfrom EarlyDarkEmu.emu import *\nfrom EarlyDarkEmu.mcmc import *\n#from EarlyDarkEmu.fisher import *\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n\n\nif_train_all = False ## Re-train all the models. Time-consuming. \nif_mcmc_all = True ## Full MCMC run. Time-consuming. \nif_savefig = False\n\nif_pk_log = True ## P(k) is to be in log10 scaling \nif_y_scale_plot_log = not if_pk_log\nif_separate_test_set = True\n\n(210,)\n\n\n\n\nLoading files\n\np_all = load_params() ## Loading Cosmological Parameters\nPk_all, k_all, z_all = load_npy_pk_k_z(pk_log_scale=if_pk_log) # Loading P(k), k and redshift\n\nprint(p_all.shape, Pk_all.shape, k_all.shape, z_all.shape, PARAM_NAME)\nprint(len(PARAM_NAME))\n\nremove_invalid_index = False\nif remove_invalid_index == True:\n    remove_sim_indx = [None]\n    valid_indices = [i for i in  np.arange(Pk_all.shape[0])  if i not in remove_sim_indx]\n\n    Pk_all = Pk_all[valid_indices]\n    p_all = p_all[valid_indices]\n\n(64, 6) (64, 51, 210) (210,) (51,) ['$\\\\omega_m$', 'h', '$\\\\sigma_8$', '$\\\\log(z_c)$', '$f_{ede}$', '$\\\\theta_i$']\n6\n\n\n\n\nA few plotting routines\n\nData loading and selection\n\nk_sdss, pk_sdss, pk_error_sdss = load_sdss()\nk_wmap, pk_wmap, pk_error_wmap = load_wmap()\nk_lya, pk_lya, pk_error_lya = load_lya()\nk_plee, pk_plee, pk_error_plee = load_planck_ee()\nk_pltt, pk_pltt, pk_error_pltt = load_planck_tt()\nk_pl, pk_pl, pk_error_pl = load_planck()\nk_des, pk_des, pk_error_des = load_des()\n\nk_lya = k_lya[k_lya &lt;= 1.0]\npk_lya = pk_lya[0:len(k_lya)]\npk_error_lya = pk_error_lya[0:len(k_lya)]\n\nk_des = k_des[k_des &lt;= 1.0]\npk_des = pk_des[0:len(k_des)]\npk_error_des = pk_error_des[0:len(k_des)]\n\n\n\nBoost metrics colored by cosmology parameters\n\ncolor_by_index = 0\nz_index = 0\n\nplot_lines_with_param_color(p_all[:, color_by_index], \n                            k_all, \n                            Pk_all[:, z_index, :], \n                            'Training data, z=' + str(z_all[z_index]), \n                            r'$k [h/Mpc]$', \n                            r'$P(k) [(Mpc/h)^3]$', \n                            PARAM_NAME[color_by_index],\n                            y_log_plot_scale=if_y_scale_plot_log);\n\nplt.ylim(0,5)\n\nplt.errorbar(k_sdss, pk_sdss, yerr=pk_error_sdss, label='SDSS DR7')\nplt.errorbar(k_wmap, pk_wmap, yerr=pk_error_wmap, label='WMAP + ACT')\nplt.errorbar(k_lya, pk_lya, yerr=pk_error_lya, label='eBOSS')\nplt.errorbar(k_plee, pk_plee, yerr=pk_error_plee, label='PLANCK_2018 EE')\nplt.errorbar(k_pltt, (pk_pltt), yerr=(pk_error_pltt), label='PLANCK_2018 TT')\nplt.errorbar(k_pl, pk_pl, yerr=pk_error_pl, label='PLANCK_2018')\nplt.errorbar(k_des, pk_des, yerr=pk_error_des, label='DES Y1 COSMIC SHEAR')\n\n\n\n\n\n\n\n\n\n\n\nTraining involves: PCA, GP fitting.\n\nif not if_separate_test_set: \n    ## Data prep\n    z_index = 0\n    y_vals = Pk_all[:, z_index, :]\n    # y_ind = np.arange(0, y_vals.shape[1])\n    y_ind = k_all\n\n    # Train-test split\n    test_indices = [60, 30, 26]\n    input_params= p_all[test_indices]\n    target_vals = Pk_all[:, z_index, :][test_indices]\n\n\n    train_indices = [i for i in  np.arange(Pk_all.shape[0]) if i not in test_indices]\n    p_all_train = p_all[train_indices]\n    y_vals_train = Pk_all[:, z_index, :][train_indices]\n    print('Redshift: ' + str(z_all[z_index]))\n\n\nif if_separate_test_set:\n\n    p_all_test = load_params(p_fileIn=LIBRARY_PARAM_FILE_TEST) \n                              ## Loading Cosmological Parameters\n    Pk_all_test, k_all_test, z_all_test = load_npy_pk_k_z(Pk_fileIn=LIBRARY_PK_FILE_TEST, \n                                                          k_fileIn=LIBRARY_K_FILE_TEST, \n                                                          z_fileIn=LIBRARY_Z_FILE_TEST,\n                                                          pk_log_scale=if_pk_log) # Loading P(k), k and redshift\n\n\n\n    ## Data prep\n    z_index = 0\n    y_vals = Pk_all[:, z_index, :]\n    # y_ind = np.arange(0, y_vals.shape[1])\n    y_ind = k_all\n\n    # Train-test split\n    test_indices = []\n    input_params= p_all_test\n    target_vals = Pk_all_test[:, z_index, :]\n\n\n    # train_indices = [i for i in  np.arange(Pk_all.shape[0]) if i not in test_indices]\n    train_indices = [i for i in  np.arange(Pk_all.shape[0])]\n    p_all_train = p_all[train_indices]\n    y_vals_train = Pk_all[:, z_index, :][train_indices]\n    print('Redshift: ' + str(z_all[z_index]))\n\nRedshift: 0.0\n\n\n\ndf_train_a = pd.DataFrame(np.concatenate([p_all, p_all_test], axis=0), columns=PARAM_NAME)\ncolors = ['lightblue']*p_all.shape[0] + ['r']*p_all_test.shape[0]\n\n# colors = ['b']*num_sims + ['r']*num_sims_test\nplot_scatter_matrix(df_train_a, colors);\n\n\n\n\n\n\n\n\n\nsepia_data = sepia_data_format(p_all_train, y_vals_train, y_ind)\nprint(sepia_data)\nmodel_filename = '../EarlyDarkEmu/model/multivariate_model_z_index' + str(z_index) \n\nsepia_model = do_pca(sepia_data, exp_variance=0.98)\nsepia_model = do_gp_train(sepia_model, model_filename)\nplot_train_diagnostics(sepia_model)\n\nThis SepiaData instance implies the following:\nThis is a simulator (eta)-only model, y dimension 210\nm  =    64 (number of simulated data)\np  =     1 (number of inputs)\nq  =     6 (number of additional simulation inputs)\npu NOT SET (transformed response dimension); call method create_K_basis \n\nStarting tune_step_sizes...\nDefault step sizes:\nbetaU\n[[0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]]\nlamUz\n[[5. 5.]]\nlamWs\n[[100. 100.]]\nlamWOs\n[[100.]]\n\n\nStep size tuning: 100%|███████████████████████| 50/50 [00:05&lt;00:00,  9.53it/s]\n\n\nDone with tune_step_size.\nSelected step sizes:\nbetaU\n[[0.20704949 0.08311083]\n [0.03600067 0.13160858]\n [0.23706296 0.15642607]\n [0.03420868 0.14280553]\n [0.00337709 0.11561421]\n [0.0051751  0.36066264]\n [0.00105523 0.03411633]]\nlamUz\n[[0.35487067 0.61483187]]\nlamWs\n[[9615.6896916  940.3264433]]\nlamWOs\n[[33.19404844]]\n\n\nMCMC sampling: 100%|█████████████████████| 1000/1000 [00:05&lt;00:00, 192.92it/s]\n\n\nModel saved to ../EarlyDarkEmu/model/multivariate_model_z_index0.pkl\nNo thetas to plot\n\n\n\n\n\n\n\n\n\n\n\nLoad existing model\n\nsepia_model = gp_load(sepia_model, model_filename)\n\nWARNING: make sure this model was instantiated with the same input data as the model corresonding to this saved model info.\n\n\n\n\nSingle-redshift emulation for new cosmological parameters\n\n# test_indices_rand = np.random.randint(size=3, low=0, high=input_params.shape[0])\ntest_indices_rand = np.arange(0, input_params.shape[0])\nprint(test_indices_rand)\npred_mean, pred_quant = emulate(sepia_model, input_params[test_indices_rand])\n# pred_quant == Emulated (0.05, 0.95) quantile\n# validation_plot(k_all, target_vals[test_indices_rand], pred_mean, pred_quant, xy_lims=[1e-5, 1e0, 10, 5e4], y_log_scale=if_y_scale_plot_log);\nvalidation_plot(k_all, target_vals[test_indices_rand], pred_mean, pred_quant, xy_lims=[1e-5, 1e0, 1, 5], y_log_plot_scale=if_y_scale_plot_log);\n\n[0 1 2 3 4 5]\n\n\n\n\n\n\n\n\n\n\n\nSensitivity analysis from the emulator\n\n# f = sensitivity_plot(k_all, p_all, sepia_model, emulate, PARAM_NAME, xy_lims = [1e-5, 1e0, 10, 5e4], y_log_plot_scale=if_y_scale_plot_log )\nf = sensitivity_plot(k_all, p_all, sepia_model, emulate, PARAM_NAME, xy_lims = [1e-5, 1e0, 1, 4.7], y_log_plot_scale=if_y_scale_plot_log )\n\n\n\n\n\n\n\n\n\n\nMulti-redshift emulation\n\nTrain all the models\n\nif if_train_all:\n    \n    do_gp_train_multiple(model_dir='../EarlyDarkEmu/model/', \n                        p_train_all = p_all[train_indices],\n                        y_vals_all = Pk_all[train_indices],\n                        y_ind_all = k_all,\n                        z_index_range=range(z_all.shape[0]))\n\n\n\nLoad all trained models\n\nsepia_model_list = load_model_multiple(model_dir='../EarlyDarkEmu/model/', \n                                        p_train_all=p_all[train_indices],\n                                        y_vals_all=Pk_all[train_indices],\n                                        y_ind_all=k_all,\n                                        z_index_range=range(z_all.shape[0]), \n                                        sepia_model_i=sepia_model)\n\nNumber of models loaded: 51\n\n\n\n\nEmulator confidence across parameter range\n\n# Parameter settings\nsteps = 20  # Number of steps in the grid for each parameter\nparam_name_extended = np.append(PARAM_NAME, 'Redshift')\nred_min = 0\nred_max = 3\nred_mean = 1.0\n\nparam_min = np.append(p_all.min(axis=0), red_min)\nparam_max = np.append(p_all.max(axis=0), red_max)\nparam_mean = np.append(p_all.mean(axis=0), red_mean)\n\n# Compute outputs and errors for a range of parameter values\ndef compute_errors(param_grid):\n    print(param_grid.shape)\n    errors = np.zeros(shape=(param_grid.shape[0], ))\n    # errors = np.array([np.mean(emu_redshift(params[np.newaxis, :], sepia_model_list, z_all)[1][:, 0, :]**2) for params in param_grid])\n    for par_indx in range(errors.shape[0]):\n\n        errors[par_indx] = np.mean(emu_redshift(param_grid[par_indx][np.newaxis, :], sepia_model_list, z_all)[1]**2)\n\n    return errors.reshape(steps, steps)\n\n\n# Example usage:\nparam_indices = [3, 4]  # Indices of parameters to vary\nfixed_indices = [i for i in range(len(param_name_extended)) if i not in param_indices]\nfixed_params = {param_name_extended[i]: param_mean[i] for i in fixed_indices}\n\nparam_grid = generate_param_grid_with_fixed(param_name_extended, param_indices, fixed_params, param_min, param_max, steps)\n\n\nerrors = compute_errors(param_grid)\n\n(400, 7)\n\n\n\nf = plot_error_heatmap(errors, [param_name_extended[i] for i in param_indices], [(param_min[param_indices[0]], param_max[param_indices[0]]), (param_min[param_indices[1]], param_max[param_indices[1]])])\n\nif if_savefig: \n    f.savefig('../../../Plots/heatmap_params_4_5.png', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\n\nParameter inference via MCMC using the emulator\n\nCreating mock observational data\n\ncreate_mock_obs = False\n\nif create_mock_obs: \n    target_indx = 0 #0 ,14, 35\n    z_index = 0\n    L = 32\n    fake_obs_data_index_every = 2\n\n    redshift = z_all[z_index]\n    x_target_mcmc = k_all[::fake_obs_data_index_every]\n    y_target_mcmc = Pk_all[:, z_index, :][target_indx][::fake_obs_data_index_every]\n\n    if not if_pk_log: y_target_mcmc = y_target_mcmc*(1 + np.random.normal(0.0, 0.5, size=y_target_mcmc.shape))\n    if if_pk_log: y_target_mcmc = y_target_mcmc*(1 + 0.02*np.random.normal(0.0, 1, size=y_target_mcmc.shape))\n\n    if not if_pk_log: yerr_target_mcmc = np.sqrt( Pk_all[:, z_index, :][target_indx][::fake_obs_data_index_every]*(L**3))/((0.1*L)**3)\n    if if_pk_log: yerr_target_mcmc = np.sqrt( Pk_all[:, z_index, :][target_indx][::fake_obs_data_index_every]*(L**3))/((0.5*L)**3)\n\n    x_grid = k_all\n    params_calib = p_all[target_indx][:, np.newaxis].T\n    print('redshift: ', redshift)\n\n\nif not create_mock_obs:\n    redshift = 0.00\n\n    k_sdss, pk_sdss, pk_error_sdss = load_sdss()\n    k_wmap, pk_wmap, pk_error_wmap = load_wmap()\n    k_lya, pk_lya, pk_error_lya = load_lya()\n    k_plee, pk_plee, pk_error_plee = load_planck_ee()\n    k_pltt, pk_pltt, pk_error_pltt = load_planck_tt()\n    k_pl, pk_pl, pk_error_pl = load_planck()\n    k_des, pk_des, pk_error_des = load_des()\n\n    k_lya = k_lya[k_lya &lt;= 1.0]\n    pk_lya = pk_lya[0:len(k_lya)]\n    pk_error_lya = pk_error_lya[0:len(k_lya)]\n    \n    k_des = k_des[k_des &lt;= 1.0]\n    pk_des = pk_des[0:len(k_des)]\n    pk_error_des = pk_error_des[0:len(k_des)]\n\n    x_target_mcmc = np.concatenate([k_sdss, k_wmap, k_lya, k_plee, k_pltt, k_pl, k_des])\n    y_target_mcmc = np.concatenate([pk_sdss, pk_wmap, pk_lya, pk_plee, pk_pltt, pk_pl, pk_des])\n\n    yerr_target_mcmc = np.concatenate([pk_error_sdss, pk_error_wmap, pk_error_lya, pk_error_plee, pk_error_pltt, pk_error_pl, pk_error_des])\n\n\n    #x_target_mcmc = k_wmap\n    #y_target_mcmc = pk_wmap\n    #yerr_target_mcmc = pk_error_wmap\n     \n    \n    params_calib = [[0.1375, 0.7, 0.8, 3.531, 0.1, 2.72]]  ## Best EDE values from 1908.06995\n    x_grid = k_all\n'''\n\nif not create_mock_obs:\n    redshift = 0.00\n\n    k_sdss, pk_sdss, pk_error_sdss = load_sdss()\n    k_wmap, pk_wmap, pk_error_wmap = load_wmap()\n    k_lya, pk_lya, pk_error_lya = load_lya()\n    k_plee, pk_plee, pk_error_plee = load_planck_ee()\n    k_pltt, pk_pltt, pk_error_pltt = load_planck_tt()\n    k_pl, pk_pl, pk_error_pl = load_planck()\n    k_des, pk_des, pk_error_des = load_des()\n\n    k_lya = k_lya[k_lya &lt;= 1.0]\n    pk_lya = pk_lya[0:len(k_lya)]\n    pk_error_lya = pk_error_lya[0:len(k_lya)]\n    \n    k_des = k_des[k_des &lt;= 1.0]\n    pk_des = pk_des[0:len(k_des)]\n    pk_error_des = pk_error_des[0:len(k_des)]\n\n    x_target_mcmc = np.concatenate([k_sdss, k_wmap, k_lya, k_plee, k_pltt, k_pl, k_des])\n    y_target_mcmc = np.concatenate([pk_sdss, pk_wmap, pk_lya, pk_plee, pk_pltt, pk_pl, pk_des])\n\n    yerr_target_mcmc = np.concatenate([pk_error_sdss, pk_error_wmap, pk_error_lya, pk_error_plee, pk_error_pltt, pk_error_pl, pk_error_des])\n\n    #x_target_mcmc = k_wmap\n    #y_target_mcmc = pk_wmap\n    #yerr_target_mcmc = pk_error_wmap\n        \n    \n    params_calib = [[0.1375, 0.7, 0.8, 3.0, 0.0, 0.0]]  ## Best EDE values from 1908.06995\n    x_grid = k_all\n'''\n\n'\\n\\nif not create_mock_obs:\\n    redshift = 0.00\\n\\n    k_sdss, pk_sdss, pk_error_sdss = load_sdss()\\n    k_wmap, pk_wmap, pk_error_wmap = load_wmap()\\n    k_lya, pk_lya, pk_error_lya = load_lya()\\n    k_plee, pk_plee, pk_error_plee = load_planck_ee()\\n    k_pltt, pk_pltt, pk_error_pltt = load_planck_tt()\\n    k_pl, pk_pl, pk_error_pl = load_planck()\\n    k_des, pk_des, pk_error_des = load_des()\\n\\n    k_lya = k_lya[k_lya &lt;= 1.0]\\n    pk_lya = pk_lya[0:len(k_lya)]\\n    pk_error_lya = pk_error_lya[0:len(k_lya)]\\n    \\n    k_des = k_des[k_des &lt;= 1.0]\\n    pk_des = pk_des[0:len(k_des)]\\n    pk_error_des = pk_error_des[0:len(k_des)]\\n\\n    x_target_mcmc = np.concatenate([k_sdss, k_wmap, k_lya, k_plee, k_pltt, k_pl, k_des])\\n    y_target_mcmc = np.concatenate([pk_sdss, pk_wmap, pk_lya, pk_plee, pk_pltt, pk_pl, pk_des])\\n\\n    yerr_target_mcmc = np.concatenate([pk_error_sdss, pk_error_wmap, pk_error_lya, pk_error_plee, pk_error_pltt, pk_error_pl, pk_error_des])\\n\\n    #x_target_mcmc = k_wmap\\n    #y_target_mcmc = pk_wmap\\n    #yerr_target_mcmc = pk_error_wmap\\n        \\n    \\n    params_calib = [[0.1375, 0.7, 0.8, 3.0, 0.0, 0.0]]  ## Best EDE values from 1908.06995\\n    x_grid = k_all\\n'\n\n\n\nf, a = plt.subplots(1,1, figsize = (8, 5)) \nz_index = 0\ninput_params_and_redshift = np.append(params_calib, redshift)\nbk_target, err_target = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, z_all)\na.plot(k_all, bk_target[:, 0], label='Emulated at target params', lw=2, ls='--', color='blue')\n\nif create_mock_obs:\n    data_label = 'Target mock observations'\nelse: \n    data_label = 'Target: SDSS + WMAP + ACT'\n\na.errorbar(x_target_mcmc, y_target_mcmc, yerr_target_mcmc, label=data_label, ls='none', lw=1, color = \"r\")\na.scatter(x_target_mcmc, y_target_mcmc, s = 5, marker = \"h\", color = \"k\")\n\n\na.plot(k_all, Pk_all[:, z_index, :].T, 'k', alpha=0.02)\n\n\nplt.plot(k_all, emulate(sepia_model_list[z_index], input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[z_index])\nplt.plot(k_all, emulate(sepia_model_list[z_index+1], input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[z_index + 1])\n\n\n\nif create_mock_obs: string_print0 = 'Target Params \\n (from sim) \\n\\n' \nif not create_mock_obs: string_print0 = 'Target params \\n(arXiv:1908.06995) \\n\\n' \n\nstring_print1 = PARAM_NAME[0] + '= %.4f'%input_params_and_redshift[0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.4f'%input_params_and_redshift[1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.4f'%input_params_and_redshift[2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.4f'%input_params_and_redshift[3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.4f'%input_params_and_redshift[4] + '\\n'\nstring_print6 = PARAM_NAME[5] + '= %.4f'%input_params_and_redshift[5] + '\\n'\nstring_print8 = 'redshift' + '= %.4f'%input_params_and_redshift[6] \n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6  + string_print8\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.1, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\n\na.set_xscale('log')\n# a.set_yscale('log')\nplt.title('pre-MCMC')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$P(k)$')\nplt.legend()\n\n\n\n\n\n\n\n\n\npos0 = chain_init(params_list, ndim, nwalkers)\n\nsampler = define_sampler(redshift, ndim, nwalkers, params_list, x_grid, sepia_model_list, z_all, x_target_mcmc, y_target_mcmc, yerr_target_mcmc)\n\n\nMCMC run - first burn, then full.\n\npos, prob, state, samples, sampler, autocorr, index = do_mcmc(sampler, pos0, nrun_burn, ndim, if_burn=True)\n\nif if_mcmc_all: # Full MCMC-run, will be slow\n    pos, prob, state, samples, sampler, autocorr, index = do_mcmc(sampler, pos, nrun, ndim, if_burn=False)\n\np_mcmc = mcmc_results(samples)\n\nfig = plot_mcmc(samples, params_list, if_truth_know=True)\n# if if_savefig: \n#     plt.savefig('../../../Plots/mcmc_plot.png', bbox_inches='tight')\n\nBurn-in phase\ntime (minutes): 0.5946133057276408\n\n\n100%|█████████████████████████████████████████| 50/50 [00:29&lt;00:00,  1.70it/s]\n\n\nSampling phase\ntime (minutes): 3.3542882164319354\n\n\n100%|███████████████████████████████████████| 400/400 [03:13&lt;00:00,  2.07it/s]\n\n\nmcmc results: 0.14501822968202113 0.6730692146483199 0.8344705259722158 3.5039653742080112 0.22362999119698324 1.6082079436390937\n\n\n\n\n\n\n\n\n\n\nf, a = plt.subplots(1,1, figsize = (8, 5)) \ninput_params_and_redshift = np.append(p_mcmc, redshift)\nbk_mcmc, err_mcmc = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, z_all)\na.plot(k_all, bk_mcmc[:, 0], label='Emulated at best MCMC', lw=2, ls='--')\na.errorbar(x_target_mcmc, y_target_mcmc, yerr_target_mcmc, label=data_label, ls='none', lw=1, color = \"r\")\na.scatter(x_target_mcmc, y_target_mcmc, s = 5, marker = \"h\", color = \"r\", alpha=0.5)\n\na.plot(k_all, Pk_all[:, z_index, :].T, 'k', alpha=0.02)\n\n\n# plt.plot(k_all, emulate(sepia_model_list[z_index], input_params_and_redshift[:-1])[0], label='Z1')\n# plt.plot(k_all, emulate(sepia_model_list[z_index+1], input_params_and_redshift[:-1])[0], label='Z2')\n\n\nstring_print1 = PARAM_NAME[0] + '= %.4f'%params_calib[0][0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.4f'%params_calib[0][1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.4f'%params_calib[0][2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.4f'%params_calib[0][3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.4f'%params_calib[0][4] + '\\n'\nstring_print6 = PARAM_NAME[5] + '= %.4f'%params_calib[0][5] + '\\n'\nstring_print8 = 'redshift' + '= %.4f'%redshift\n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6  + string_print8\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.5, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\nstring_print0_mcmc = 'Optimized Params \\n\\n' \nstring_print1_mcmc = PARAM_NAME[0] + '= %.4f'%p_mcmc[0] + '\\n'\nstring_print2_mcmc = PARAM_NAME[1] + '= %.4f'%p_mcmc[1] + '\\n'\nstring_print3_mcmc = PARAM_NAME[2] + '= %.4f'%p_mcmc[2] + '\\n'\nstring_print4_mcmc = PARAM_NAME[3] + '= %.4f'%p_mcmc[3] + '\\n'\nstring_print5_mcmc = PARAM_NAME[4] + '= %.4f'%p_mcmc[4] + '\\n'\nstring_print6_mcmc = PARAM_NAME[5] + '= %.4f'%p_mcmc[5]\n\nstring_print_mcmc = string_print0_mcmc + string_print1_mcmc + string_print2_mcmc + string_print3_mcmc + string_print4_mcmc + string_print5_mcmc + string_print6_mcmc \n\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nplt.text(1.02, -0.05, string_print_mcmc, transform=a.transAxes, fontsize=12, bbox=props)\n\n\n\na.set_xscale('log')\n# a.set_yscale('log')\nplt.title('P(k) at MCMC constraints')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$P(k)$')\nplt.legend()\n\nif if_savefig: \n    plt.savefig('../../../Plots/mcmc_results_Bk.png', bbox_inches='tight')\n'''\nf, a = plt.subplots(1,1, figsize = (8, 5)) \np_mcmc_ede = np.append(p_mcmc, (3, 0, 0))\ninput_params_and_redshift = np.append(p_mcmc_ede,redshift)\nbk_mcmc, err_mcmc = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, z_all)\na.plot(k_all, bk_mcmc[:, 0], label='Emulated at best MCMC', lw=2, ls='--')\na.errorbar(x_target_mcmc, y_target_mcmc, yerr_target_mcmc, label=data_label, ls='none', lw=1, color = \"r\")\na.scatter(x_target_mcmc, y_target_mcmc, s = 5, marker = \"h\", color = \"r\", alpha=0.5)\n\na.plot(k_all, Pk_all[:, z_index, :].T, 'k', alpha=0.02)\n\n\n# plt.plot(k_all, emulate(sepia_model_list[z_index], input_params_and_redshift[:-1])[0], label='Z1')\n# plt.plot(k_all, emulate(sepia_model_list[z_index+1], input_params_and_redshift[:-1])[0], label='Z2')\n\n\nstring_print1 = PARAM_NAME[0] + '= %.4f'%params_calib[0][0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.4f'%params_calib[0][1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.4f'%params_calib[0][2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.4f'%params_calib[0][3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.4f'%params_calib[0][4] + '\\n'\nstring_print6 = PARAM_NAME[5] + '= %.4f'%params_calib[0][5] + '\\n'\nstring_print8 = 'redshift' + '= %.4f'%redshift\n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6  + string_print8\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.5, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\nstring_print0_mcmc = 'Optimized Params \\n\\n' \nstring_print1_mcmc = PARAM_NAME[0] + '= %.4f'%p_mcmc_ede[0] + '\\n'\nstring_print2_mcmc = PARAM_NAME[1] + '= %.4f'%p_mcmc_ede[1] + '\\n'\nstring_print3_mcmc = PARAM_NAME[2] + '= %.4f'%p_mcmc_ede[2] + '\\n'\nstring_print4_mcmc = PARAM_NAME[3] + '= %.4f'%p_mcmc_ede[3] + '\\n'\nstring_print5_mcmc = PARAM_NAME[4] + '= %.4f'%p_mcmc_ede[4] + '\\n'\nstring_print6_mcmc = PARAM_NAME[5] + '= %.4f'%p_mcmc_ede[5]\n\nstring_print_mcmc = string_print0_mcmc + string_print1_mcmc + string_print2_mcmc + string_print3_mcmc + string_print4_mcmc + string_print5_mcmc + string_print6_mcmc \n\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nplt.text(1.02, -0.05, string_print_mcmc, transform=a.transAxes, fontsize=12, bbox=props)\n\n\n\na.set_xscale('log')\n# a.set_yscale('log')\nplt.title('P(k) at MCMC constraints')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$P(k)$')\nplt.legend()\n\nif if_savefig: \n    plt.savefig('../../../Plots/mcmc_results_Bk.png', bbox_inches='tight')\n'''\n\n'\\nf, a = plt.subplots(1,1, figsize = (8, 5)) \\np_mcmc_ede = np.append(p_mcmc, (3, 0, 0))\\ninput_params_and_redshift = np.append(p_mcmc_ede,redshift)\\nbk_mcmc, err_mcmc = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, z_all)\\na.plot(k_all, bk_mcmc[:, 0], label=\\'Emulated at best MCMC\\', lw=2, ls=\\'--\\')\\na.errorbar(x_target_mcmc, y_target_mcmc, yerr_target_mcmc, label=data_label, ls=\\'none\\', lw=1, color = \"r\")\\na.scatter(x_target_mcmc, y_target_mcmc, s = 5, marker = \"h\", color = \"r\", alpha=0.5)\\n\\na.plot(k_all, Pk_all[:, z_index, :].T, \\'k\\', alpha=0.02)\\n\\n\\n# plt.plot(k_all, emulate(sepia_model_list[z_index], input_params_and_redshift[:-1])[0], label=\\'Z1\\')\\n# plt.plot(k_all, emulate(sepia_model_list[z_index+1], input_params_and_redshift[:-1])[0], label=\\'Z2\\')\\n\\n\\nstring_print1 = PARAM_NAME[0] + \\'= %.4f\\'%params_calib[0][0] + \\'\\n\\'\\nstring_print2 = PARAM_NAME[1] + \\'= %.4f\\'%params_calib[0][1] + \\'\\n\\'\\nstring_print3 = PARAM_NAME[2] + \\'= %.4f\\'%params_calib[0][2] + \\'\\n\\'\\nstring_print4 = PARAM_NAME[3] + \\'= %.4f\\'%params_calib[0][3] + \\'\\n\\'\\nstring_print5 = PARAM_NAME[4] + \\'= %.4f\\'%params_calib[0][4] + \\'\\n\\'\\nstring_print6 = PARAM_NAME[5] + \\'= %.4f\\'%params_calib[0][5] + \\'\\n\\'\\nstring_print8 = \\'redshift\\' + \\'= %.4f\\'%redshift\\n\\n\\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6  + string_print8\\n\\nprops = dict(boxstyle=\\'round\\', facecolor=\\'gray\\', alpha=0.2)\\nplt.text(1.02, 0.5, string_print, transform=a.transAxes, fontsize=12, bbox=props)\\n\\nstring_print0_mcmc = \\'Optimized Params \\n\\n\\' \\nstring_print1_mcmc = PARAM_NAME[0] + \\'= %.4f\\'%p_mcmc_ede[0] + \\'\\n\\'\\nstring_print2_mcmc = PARAM_NAME[1] + \\'= %.4f\\'%p_mcmc_ede[1] + \\'\\n\\'\\nstring_print3_mcmc = PARAM_NAME[2] + \\'= %.4f\\'%p_mcmc_ede[2] + \\'\\n\\'\\nstring_print4_mcmc = PARAM_NAME[3] + \\'= %.4f\\'%p_mcmc_ede[3] + \\'\\n\\'\\nstring_print5_mcmc = PARAM_NAME[4] + \\'= %.4f\\'%p_mcmc_ede[4] + \\'\\n\\'\\nstring_print6_mcmc = PARAM_NAME[5] + \\'= %.4f\\'%p_mcmc_ede[5]\\n\\nstring_print_mcmc = string_print0_mcmc + string_print1_mcmc + string_print2_mcmc + string_print3_mcmc + string_print4_mcmc + string_print5_mcmc + string_print6_mcmc \\n\\nprops = dict(boxstyle=\\'round\\', facecolor=\\'blue\\', alpha=0.2)\\nplt.text(1.02, -0.05, string_print_mcmc, transform=a.transAxes, fontsize=12, bbox=props)\\n\\n\\n\\na.set_xscale(\\'log\\')\\n# a.set_yscale(\\'log\\')\\nplt.title(\\'P(k) at MCMC constraints\\')\\na.set_xlabel(r\\'$k [h/Mpc]$\\')\\na.set_ylabel(r\\'$P(k)$\\')\\nplt.legend()\\n\\nif if_savefig: \\n    plt.savefig(\\'../../../Plots/mcmc_results_Bk.png\\', bbox_inches=\\'tight\\')\\n'\n\n\n\n\n\n\n\n\n\n\nnp.savetxt('pk_ede', bk_mcmc, fmt='%f', delimiter='\\t')",
    "crumbs": [
      "EarlyDarkEmu"
    ]
  },
  {
    "objectID": "index.html#fisher-matrix-analysis",
    "href": "index.html#fisher-matrix-analysis",
    "title": "EarlyDarkEmu",
    "section": "Fisher matrix analysis",
    "text": "Fisher matrix analysis\n\n#Fisher Class\nimport numpy as np\nfrom scipy.integrate import simps\n\n\ndef power_spectra(params, redshift):\n    params_and_redshift = np.append(params, redshift)\n    pred_mean, pred_err = emu_redshift(params_and_redshift[np.newaxis, :], sepia_model_list, z_all)\n    return 10**pred_mean[:, 0], 10**pred_err[:, 0] ## Check how to scale P(k) and Pk_errors properly\n\n\nclass FisherMatrix:\n    def __init__(self, xvals, yvals, y_errors, params):\n        self.xvals = np.array(xvals)\n        self.yvals = np.array(yvals)\n        self.y_errors = np.array(y_errors)\n        self.params = np.array(params)\n        # self.redshift = self.redshift\n        self.n_params = len(params)\n        self.fmat = np.zeros((self.n_params, self.n_params))\n\n    def compute_fmat(self, derivatives):\n        for i in range(self.n_params):\n            for j in range(self.n_params):\n                integrand = derivatives[i] * derivatives[j] / self.y_errors**2\n                # self.fmat[i, j] = np.sum(integrand)\n                self.fmat[i, j] = simps(integrand, self.xvals)\n        return self.fmat\n\n    def uncertainty(self):\n        if np.linalg.det(self.fmat) == 0:\n            raise ValueError(\"Fisher is singular.\")\n        inv_fisher = np.linalg.inv(self.fmat)\n        return np.sqrt(np.diagonal(inv_fisher))\n    \n\ndef deriv(params_array, redshift, forward_func, perturb = 1e-5):\n    n_par = len(params_array)\n    yvals, _ = forward_func(params_array, redshift)\n    derivative = np.zeros((n_par, len(yvals)))\n\n    for i in range(n_par):\n        params_up  = params_array.copy()\n        params_down = params_array.copy()\n\n        params_up[i] += perturb\n        params_down[i] -= perturb\n\n        yvals_up, _ = forward_func(params_up, redshift) \n        yvals_down, _ = forward_func(params_down, redshift)\n\n        derivative[i] = (yvals_up - yvals_down)/(2*perturb)\n        \n    return derivative, yvals_up, yvals_down\n\n\n#need to plot the 2x2 hessian ellipses - Function from Nesar\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nimport numpy as np\n\n\ndef plot_contours(hessian, pos, nstd=1., ax=None, **kwargs):\n    def eigsorted(cov):\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        return vals[order], vecs[:, order]\n\n    # Ensure Hessian is negative definite for covariance inversion\n    if np.all(np.linalg.eigvals(-hessian) &gt; 0):\n        mat = -hessian\n    else:\n        #raise ValueError(\"The Hessian is not negative definite.\")\n        epsilon = 1e-5  # A small positive value\n        mat = hessian - epsilon * np.eye(hessian.shape[0])\n    cov = np.linalg.pinv(mat)\n    \n    # Check for valid covariance values\n    if not np.all(np.isfinite(cov)):\n        raise ValueError(\"Covariance matrix contains NaN or Inf.\")\n\n    sigma_marg = lambda i: np.sqrt(cov[i, i])\n\n    if ax is None:\n        ax = plt.gca()\n\n    vals, vecs = eigsorted(cov)\n    if np.any(vals &lt; 0):\n        raise ValueError(\"Negative eigenvalues found in covariance matrix.\")\n\n    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n\n    # Width and height are \"full\" widths, not radius\n    width, height = 2 * nstd * np.sqrt(np.abs(vals))\n    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n\n    ax.add_artist(ellip)\n    sz = max(width, height)\n    s1 = 1.5 * nstd * sigma_marg(0)\n    s2 = 1.5 * nstd * sigma_marg(1)\n\n    ax.axhline(pos[1], color='blue')\n    ax.axvline(pos[0], color='blue')\n\n    ax.set_xlim(pos[0] - s1, pos[0] + s1)\n    ax.set_ylim(pos[1] - s2, pos[1] + s2)\n    plt.draw()\n    return ellip\n\n###################################################################\n\ndef plot_fisher_grid_aligned(param_names, param_fiducial, fisher_matrix_6x6):\n    n_params = len(param_names)\n    fig, axes = plt.subplots(nrows=n_params, ncols=n_params, figsize=(12, 10), sharex='col', sharey='row')\n    fig.subplots_adjust(top=0.9, right=0.9, left=0.0, bottom=0.0, hspace=0.01, wspace=0.01)\n\n    neg_fmat = -fisher_matrix_6x6\n\n    for i in range(n_params):\n        for j in range(n_params):\n            if i &gt; j:\n                ax = axes[i, j]\n                param_duo = np.array([param_fiducial[j], param_fiducial[i]])\n                fish2x2 = np.array([[neg_fmat[j, j], neg_fmat[j, i]], [neg_fmat[i, j], neg_fmat[i, i]]])\n                \n                plot_contours(fish2x2, param_duo, nstd=1., ax=ax, alpha=0.4, fill=True, edgecolor='red', linewidth=2)\n\n                if j == 0:\n                    ax.set_ylabel(param_names[i], fontsize=12)\n                if i == n_params - 1:\n                    ax.set_xlabel(param_names[j], fontsize=12)\n            else:\n                axes[i, j].axis('off')\n\n    fig.tight_layout()\n    fig.show()\n\n\ndef plot_fisher_grid_multiple(param_names, param_fiducial, fisher_matrices, matrix_labels):\n    n_params = len(param_names)\n    n_matrices = len(fisher_matrices)\n\n    fig, axes = plt.subplots(nrows=n_params, ncols=n_params, figsize=(12, 10), sharex='col', sharey='row')\n    plt.subplots_adjust(top=0.90, right=0.85, left=0.08, bottom=0.08, hspace=0.4, wspace=0.4)\n\n    colors = ['red', 'blue', 'green', 'purple']  # Adjust as needed\n    legend_handles = []\n\n    for i in range(n_params):\n        for j in range(n_params):\n            if i &gt; j:\n                ax = axes[i, j]\n                param_duo = np.array([param_fiducial[j], param_fiducial[i]])\n\n                # Plot ellipses for each Fisher matrix\n                for k, (fisher_matrix, label) in enumerate(zip(fisher_matrices, matrix_labels)):\n                    neg_fmat = -fisher_matrix\n                    fish2x2 = np.array([[neg_fmat[j, j], neg_fmat[j, i]], \n                                        [neg_fmat[i, j], neg_fmat[i, i]]])\n\n                    ellip = plot_contours(fish2x2, param_duo, nstd=1., ax=ax, \n                                          alpha=0.3, fill=True, edgecolor=colors[k], \n                                          linewidth=2)\n\n                    if i == n_params - 1 and j == 0:\n                        legend_handles.append((ellip, label))  # Collect handles and labels\n\n                if j == 0:\n                    ax.set_ylabel(param_names[i], fontsize=12)\n                if i == n_params - 1:\n                    ax.set_xlabel(param_names[j], fontsize=12)\n\n            else:\n                axes[i, j].axis('off')\n\n    # Create a single legend outside the plot\n    # fig.legend(*zip(*legend_handles), loc='center right', fontsize=10)\n    fig.legend(*zip(*legend_handles), loc='center', fontsize=14, \n               bbox_to_anchor=(0.7, 0.7), ncol=1, frameon=True)\n\n    plt.tight_layout()\n    plt.show()\n\n\n#We need to plot the 2x2 hessian ellipses - Function from Nesar\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nimport numpy as np\nstandalone_fisher_test = False\n\nif standalone_fisher_test: \n        param_fiducial = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\n        p_names = ['$\\\\omega_m$', 'h', '$\\\\sigma_8$', '$\\\\log(z_c)$', '$f_{ede}$', '$\\\\theta_i$']\n        p_min = np.array([0.12, 0.55, 0.7 , 3.  , 0.  , 0.  ])\n        p_max = np.array([0.155, 0.85 , 0.9  , 4.   , 0.5  , 3.14 ])\n\n        fmat_fid = np.array([[  33602.33436514,  -16812.85135088,  -33306.49447045,\n         -28651.99102799,    7955.28501939,  -19192.39592227],\n       [ -16812.85135088,    8471.49395214,   12617.76405109,\n          12309.1688204 ,    -599.4785199 ,    9966.86094666],\n       [ -33306.49447045,   12617.76405109,  309579.52928037,\n         166905.49266398, -238929.54284678,   -5850.67462856],\n       [ -28651.99102799,   12309.1688204 ,  166905.49266398,\n          93795.30933078, -122491.4294374 ,    3907.87406654],\n       [   7955.28501939,    -599.4785199 , -238929.54284678,\n        -122491.4294374 ,  194898.50173265,   16236.1357093 ],\n       [ -19192.39592227,    9966.86094666,   -5850.67462856,\n           3907.87406654,   16236.1357093 ,   13199.13065175]])\n\nelse: \n        param_fiducial = p_mcmc\n        p_names = PARAM_NAME\n        p_min = allMin\n        p_max = allMax\n\n\ndef plot_contours(hessian, pos, nstd=1., ax=None, **kwargs):\n    def eigsorted(cov):\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        return vals[order], vecs[:, order]\n\n    # Ensure Hessian is negative definite for covariance inversion\n    if np.all(np.linalg.eigvals(-hessian) &gt; 0):\n        mat = -hessian\n    else:\n        raise ValueError(\"The Hessian is not negative definite.\")\n    cov = np.linalg.pinv(mat)\n    \n    # Check for valid covariance values\n    if not np.all(np.isfinite(cov)):\n        raise ValueError(\"Covariance matrix contains NaN or Inf.\")\n\n    sigma_marg = lambda i: np.sqrt(cov[i, i])\n\n    if ax is None:\n        ax = plt.gca()\n\n    vals, vecs = eigsorted(cov)\n    if np.any(vals &lt; 0):\n        raise ValueError(\"Negative eigenvalues found in covariance matrix.\")\n\n    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n\n    # Width and height are \"full\" widths, not radius\n    width, height = 2 * nstd * np.sqrt(np.abs(vals))\n    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n\n    ax.add_artist(ellip)\n    sz = max(width, height)\n    s1 = 1.5 * nstd * sigma_marg(0)\n    s2 = 1.5 * nstd * sigma_marg(1)\n\n    ax.axhline(pos[1], color='blue')\n    ax.axvline(pos[0], color='blue')\n\n    ax.set_xlim(pos[0] - s1, pos[0] + s1)\n    ax.set_ylim(pos[1] - s2, pos[1] + s2)\n    plt.draw()\n    return ellip\n\n###################################################################\n\ndef plot_fisher_grid_aligned(param_names, param_fiducial, fisher_matrix_6x6):\n    n_params = len(param_names)\n    fig, axes = plt.subplots(nrows=n_params, ncols=n_params, figsize=(16, 14), sharex='col', sharey='row')\n    fig.subplots_adjust(top=0.9, right=0.9, left=0.0, bottom=0.0, hspace=0.01, wspace=0.01)\n\n    neg_fmat = -fisher_matrix_6x6\n\n    for i in range(n_params):\n        for j in range(n_params):\n            if i &gt; j:\n                ax = axes[i, j]\n                param_duo = np.array([param_fiducial[j], param_fiducial[i]])\n                fish2x2 = np.array([[neg_fmat[j, j], neg_fmat[j, i]], [neg_fmat[i, j], neg_fmat[i, i]]])\n                \n                plot_contours(fish2x2, param_duo, nstd=1., ax=ax, alpha=0.4, fill=True, edgecolor='red', linewidth=2)\n\n                if j == 0:\n                    ax.set_ylabel(param_names[i], fontsize=12)\n                if i == n_params - 1:\n                    ax.set_xlabel(param_names[j], fontsize=12)\n            else:\n                axes[i, j].axis('off')\n\n    fig.tight_layout()\n    fig.show()\n\n\ndef plot_fisher_grid_multiple(param_names, param_fiducial, fisher_matrices, matrix_labels):\n    n_params = len(param_names)\n    n_matrices = len(fisher_matrices)\n\n    fig, axes = plt.subplots(nrows=n_params, ncols=n_params, figsize=(16, 14), sharex='col', sharey='row')\n    plt.subplots_adjust(top=0.90, right=0.85, left=0.08, bottom=0.08, hspace=0.4, wspace=0.4)\n\n    colors = ['red', 'blue', 'green', 'purple']  # Adjust as needed\n    legend_handles = []\n\n    for i in range(n_params):\n        for j in range(n_params):\n            if i &gt; j:\n                ax = axes[i, j]\n                param_duo = np.array([param_fiducial[j], param_fiducial[i]])\n\n                # Plot ellipses for each Fisher matrix\n                for k, (fisher_matrix, label) in enumerate(zip(fisher_matrices, matrix_labels)):\n                    neg_fmat = -fisher_matrix\n                    fish2x2 = np.array([[neg_fmat[j, j], neg_fmat[j, i]], \n                                        [neg_fmat[i, j], neg_fmat[i, i]]])\n\n                    ellip = plot_contours(fish2x2, param_duo, nstd=1., ax=ax, \n                                          alpha=0.3, fill=True, edgecolor=colors[k], \n                                          linewidth=2)\n\n                    if i == n_params - 1 and j == 0:\n                        legend_handles.append((ellip, label))  # Collect handles and labels\n\n                if j == 0:\n                    ax.set_ylabel(param_names[i], fontsize=12)\n                if i == n_params - 1:\n                    ax.set_xlabel(param_names[j], fontsize=12)\n\n            else:\n                axes[i, j].axis('off')\n\n    # Create a single legend outside the plot\n    # fig.legend(*zip(*legend_handles), loc='center right', fontsize=10)\n    fig.legend(*zip(*legend_handles), loc='center', fontsize=14, \n               bbox_to_anchor=(0.5, 0.7), ncol=1, frameon=True)\n\n    plt.tight_layout()\n    plt.show()\n\n\nredshift_fid_allk = 0.01\nparam_fid_allk = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\n\n\npk_fid_allk, pk_error_fid_allk = power_spectra(param_fid_allk, redshift_fid_allk)\npk_error_added_allk = 1* np.random.uniform(low=0.5, high=2.2, size=pk_fid_allk.shape[0]) + 0.2*pk_fid_allk* np.random.uniform(low=0.9, high=1.1, size=pk_fid_allk.shape[0])\n\npk_error_total_allk = pk_error_fid_allk + pk_error_added_allk ## instead of pk_error_fid\n\nderivs_allk, pk_up_allk, pk_down_allk = deriv(param_fid_allk, redshift_fid_allk, power_spectra)\nk_values_allk = k_all\nfisher_allk = FisherMatrix(k_values_allk, pk_fid_allk, pk_error_total_allk, param_fid_allk)\nfmat_fid_allk = fisher_allk.compute_fmat(derivs_allk)\nfisher_err_allk = fisher_allk.uncertainty()\n#fish_all = np.array(_2x2_matrices(-fmat_fid))\n\n\nplot_fisher_grid_aligned(p_names, param_fid_allk, fmat_fid_allk)\n\n\n\n\n\n\n\n\n\n## Different redshifts\n\n\nredshift_fid_zbin = 0.9\nparam_fid_zbin = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\n\ndef computs_fisher_mat_zbin(redshift_fid):\n\n    pk_fid_zbin, pk_error_fid_zbin = power_spectra(param_fid_zbin, redshift_fid)\n    pk_error_added_zbin = 1* np.random.uniform(low=0.5, high=2.2, size=pk_fid_zbin.shape[0]) + 0.2*pk_fid_zbin* np.random.uniform(low=0.9, high=1.1, size=pk_fid_zbin.shape[0])\n    \n    pk_error_total_zbin = pk_error_fid_zbin + pk_error_added_zbin ## instead of pk_error_fid\n    \n    derivs_zbin, pk_up_zbin, pk_down_zbin = deriv(param_fid_zbin, redshift_fid, power_spectra)\n    k_values_zbin = k_all\n    fisher_zbin = FisherMatrix(k_values_zbin, pk_fid_zbin, pk_error_total_zbin, param_fid_zbin)\n    fmat_fid_zbin = fisher_zbin.compute_fmat(derivs_zbin)\n    #fish_all = np.array(_2x2_matrices(-fmat_fid))\n\n    return fmat_fid_zbin\n\nfmat_fid_z01 = computs_fisher_mat_zbin(redshift_fid = 0.01)\nfmat_fid_z05 = computs_fisher_mat_zbin(redshift_fid = 0.25)\nfmat_fid_z10 = computs_fisher_mat_zbin(redshift_fid = 0.5)\n\n\n# Example usage:\nfisher_matrices_zbin = [fmat_fid_z01, fmat_fid_z05, fmat_fid_z10] \nmatrix_labels_zbin = [r'$z=0.01$', r'$z=0.25$', r'$z=0.5$']\nplot_fisher_grid_multiple(p_names, param_fid_zbin, fisher_matrices_zbin, matrix_labels_zbin)\n\n\n\n\n\n\n\n\n\nNew Stuff(different k-ranges) – needs to be cleaned up - DONE\n\nimport numpy as np\n\ndef compute_fisher_k(k_vals, perturb=1e-5):\n\n    len_k = len(k_vals)\n    \n    pk_slice = np.log(pk_fid_allk[0:len_k-1])\n    pk_error_slice = np.log(pk_error_fid_allk[0:len_k-1])\n    k_slice = k_all[0:len_k-1]\n    \n    # Add random errors to pk_error_slice\n    pk_error_added_kbin = 1 * np.random.uniform(low=0.5, high=2.2, size=pk_slice.shape[0]) + 0.2 * pk_slice * np.random.uniform(low=0.9, high=1.1, size=pk_slice.shape[0])\n    pk_error_total_kbin = pk_error_slice + pk_error_added_kbin  # Instead of pk_error_fid\n\n    # Initialize the derivatives array\n    derivs_kbin = np.zeros((len(pk_slice), len(pk_slice)))\n\n    # Compute derivatives for the Fisher matrix\n    for i in range(len(param_fid_allk)):\n        params_up_kbin = param_fid_allk.copy()\n        params_down_kbin = param_fid_allk.copy()\n\n        params_up_kbin[i] += perturb\n        params_down_kbin[i] -= perturb\n\n        # Compute the power spectra for perturbed parameters\n        pk_up_kbin, _ = power_spectra(params_up_kbin, redshift_fid_allk)\n        pk_down_kbin, _ = power_spectra(params_down_kbin, redshift_fid_allk)\n\n        # Compute the derivative (finite difference)\n        derivs_kbin[i] = (pk_up_kbin[0:len_k-1] - pk_down_kbin[0:len_k-1]) / (2 * perturb)\n\n    # Create Fisher matrix instance and compute the matrix\n    fisher_kbin = FisherMatrix(k_slice, pk_slice, pk_error_total_kbin, param_fid_allk)\n    fmat_kbin = fisher_kbin.compute_fmat(derivs_kbin)\n\n    return fmat_kbin\n\n\nfmat_01_n = compute_fisher_k(k_all[0:85])\nfmat_02_n = compute_fisher_k(k_all[85:128])\nfmat_03_n = compute_fisher_k(k_all[128:170])\nfmat_04_n = compute_fisher_k(k_all[170:210])\n\n\nfisher_matrices_n = [fmat_01_n, fmat_02_n, fmat_03_n, fmat_04_n] \nmatrix_labels_n = [r'$1e-5 \\leq k &lt; 1e-3h/Mpc$', r'$1e-3h/Mpc \\leq k &lt; 1e-2h/Mpc$', r'$1e-2h/Mpc \\leq k &lt; 1e-1h/Mpc$', r'$1e-1h/Mpc \\leq k &lt; 1e0h/Mpch/Mpc$']\nplot_fisher_grid_multiple(p_names, param_fid_allk, fisher_matrices_n, matrix_labels_n)\n\n\n#plt.errorbar(k_all, pk_fid, yerr = pk_error_added)\nplt.errorbar(k_all, pk_fid_allk, yerr = pk_error_fid_allk * 100, marker = '.', ls = 'None')\nplt.yscale('log')\n\n\n\nInterpolation of P(k) from survey\n\n#k_interp = np.concatenate([k_pltt, k_pl, k_plee, k_sdss, k_wmap, k_des, k_lya]) \n#pk_error_interp = np.concatenate([pk_error_pltt, pk_error_pl, pk_error_plee, pk_error_sdss, pk_error_wmap, pk_error_des, pk_error_lya])\nk_interp = k_wmap\nredshift_fid_i = 0.01\nparam_fid_i = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\npk_fid_i, _ = power_spectra(param_fid_i, redshift_fid_i)\npk_interp = np.interp(k_interp, k_all, pk_fid_i)\npk_error_interp  = pk_error_wmap\n\n\nplt.scatter(k_interp, pk_interp)\nplt.xlabel('k, all survey data')\nplt.ylabel('P(k) - interpolated')\n\n\nplt.errorbar(k_interp, pk_interp, yerr = pk_error_interp, marker = 'o', color = 'r', alpha = 0.5, ls = 'None')\nplt.xlabel('k, all survey data')\nplt.ylabel('P(k) - interpolated')\n\n\nplt.errorbar(k_interp, pk_interp, yerr = pk_error_interp, marker = 'o', color = 'r', alpha = 0.5, ls = 'None')\n#plt.errorbar(k_all[110:160], pk_fid_allk[110:160], yerr = pk_error_fid_allk[110:160] * 100, marker = 'o', alpha = 0.3, ls = 'None')\nplt.errorbar(k_all, pk_fid_allk, yerr = pk_error_fid_allk * 100, marker = 'o', alpha = 0.3, ls = 'None')\n\nFisher with Interpolated Pk\n\nredshift_fid_i = 0.02\nparam_fid_i = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\n\n\npk_fid_i,_ = power_spectra(param_fid_i, redshift_fid_i)\npk_interp = np.interp(k_interp, k_all, pk_fid_i)\npk_error_added_interp = 1* np.random.uniform(low=0.5, high=2.2, size=pk_interp.shape[0]) + 0.2*pk_interp* np.random.uniform(low=0.9, high=1.1, size=pk_interp.shape[0])\n\npk_error_total = pk_error_interp + pk_error_added_interp ## instead of pk_error_fid\n\n\nperturb = 1e-5\nderivs_interp = np.zeros((len(param_fid_i), len(pk_interp)))\n\nfor i in range(len(param_fid_i)):\n    params_up_interp  = param_fid_i.copy()\n    params_down_interp  = param_fid_i.copy()\n\n    params_up_interp[i] += perturb\n    params_down_interp[i] -= perturb\n\n    # pk_up = power_spectra(kvals, params_up)\n    # pk_down = power_spectra(kvals, params_down)\n    pk_up, _ = power_spectra(params_up_interp, redshift)\n    pk_up_interp = np.interp(k_interp, k_all, pk_up)\n    pk_down, _ = power_spectra(params_down_interp, redshift)\n    pk_down_interp = np.interp(k_interp, k_all, pk_down)\n    \n    derivs_interp[i] = (pk_up_interp - pk_down_interp)/(2*perturb)\n\nk_values = k_interp\nfisher_interp = FisherMatrix(k_values, pk_interp, pk_error_total, param_fid_i)\nfmat_fid_interp = fisher_interp.compute_fmat(derivs_interp) \n#fish_all_interp = np.array(_2x2_matrices(-fmat_fid))\n\n\nplot_fisher_grid_aligned(p_names, param_fid_i, fmat_fid_interp)\n\n\nparam_fid_i_zbin = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\n\ndef computs_fisher_mat(redshift_input):\n\n    pk_fid_i_zbin,_ = power_spectra(param_fid_i, redshift_input)\n    pk_interp_zbin = np.interp(k_interp, k_all, pk_fid_i_zbin)\n    \n    pk_error_added_interp_zbin = 1* np.random.uniform(low=0.5, high=2.2, size=pk_interp_zbin.shape[0]) + 0.2*pk_interp_zbin* np.random.uniform(low=0.9, high=1.1, size=pk_interp_zbin.shape[0])\n    pk_error_total_zbin = pk_error_interp + pk_error_added_interp_zbin ## instead of pk_error_fid\n\n\n    perturb = 1e-5\n    derivs_interp_zbin = np.zeros((len(pk_interp_zbin), len(pk_interp_zbin)))\n\n    for i in range(len(param_fid_i_zbin)):\n        params_up_interp_zbin  = param_fid_i_zbin.copy()\n        params_down_interp_zbin  = param_fid_i_zbin.copy()\n    \n        params_up_interp_zbin[i] += perturb\n        params_down_interp_zbin[i] -= perturb\n    \n        # pk_up = power_spectra(kvals, params_up)\n        # pk_down = power_spectra(kvals, params_down)\n        pk_up_i_zbin, _ = power_spectra(params_up_interp_zbin, redshift_input)\n        pk_up_interp_zbin = np.interp(k_interp, k_all, pk_up_i_zbin)\n        pk_down_i_zbin, _ = power_spectra(params_down_interp_zbin, redshift_input)\n        pk_down_interp_zbin = np.interp(k_interp, k_all, pk_down_i_zbin)\n    \n        derivs_interp_zbin[i] = (pk_up_interp_zbin - pk_down_interp_zbin)/(2*perturb)\n\n    k_values = k_interp\n    fisher_interp_zbin = FisherMatrix(k_values, pk_interp_zbin, pk_error_total_zbin, param_fid_i_zbin)\n    fmat_fid_interp_zbin = fisher_interp_zbin.compute_fmat(derivs_interp_zbin) \n    #fish_all = np.array(_2x2_matrices(-fmat_fid))\n\n    return fmat_fid_interp_zbin\n\nparam_fid = np.array([0.13844, 0.6991, 0.7997, 3.530, 0.0993, 2.72])\n\nfmat_fid_z01_interp = computs_fisher_mat(redshift_input = 0.01)\nfmat_fid_z05_interp = computs_fisher_mat(redshift_input = 0.25)\nfmat_fid_z10_interp = computs_fisher_mat(redshift_input = 0.5)\n\n\nfisher_matrices = [fmat_fid_z01_interp, fmat_fid_z05_interp, fmat_fid_z10_interp] \nmatrix_labels = [r'$z=0.01$', r'$z=0.25$', r'$z=0.5$']\nplot_fisher_grid_multiple(p_names, param_fiducial, fisher_matrices, matrix_labels)\n\n\ndef deriv(forward_func, perturb = 1e-5, *args, **kwargs):\n    n_par = len(params_array)\n    yvals, _ = forward_func(*args, **kwargs)\n    derivative = np.zeros((len(yvals), len(yvals)))\n\n    for i in range(n_par):\n        params_up  = params_array.copy()\n        params_down = params_array.copy()\n\n        params_up[i] += perturb\n        params_down[i] -= perturb\n\n        # hmf_up = hmf(kvals, params_up)\n        # hmf_down = hmf(kvals, params_down)\n        yvals_up, _ = forward_func(*args, **kwargs) \n        yvals_down, _ = forward_func(*args, **kwargs)\n\n        derivative[i] = (yvals_up - yvals_down)/(2*perturb)\n        \n    return derivative, yvals_up, yvals_down\n\n\nimport pygtc\ndef plot_mcmc_new(samples: np.array, \n              params_list: list, \n              if_truth_know: bool = False, \n              ax: plt.Axes = None):\n              \n    # Extract parameter names and truths, assuming params_list structure as\n    # [(name, truth, min_range, max_range), ...]\n    param_names = [param[0] for param in params_list]\n    truths = [param[1] for param in params_list] if if_truth_know else None\n    param_ranges = [(param[2], param[3]) for param in params_list]\n\n    # Configure the plot settings\n    fig = pygtc.plotGTC(samples, \n                        paramNames=param_names,\n                        truths=truths,\n                        figureSize=8,\n                        plotDensity=True,\n                        filledPlots=True,\n                        smoothingKernel=3,\n                        nContourLevels=3,\n                        customLabelFont={'family': 'DejaVu Sans', 'size': 12},\n                        customTickFont={'family': 'DejaVu Sans', 'size': 12},\n                        # paramRanges=param_ranges\n                        ax=ax  # Pass the existing axis to the plot\n                        )\n    \n    return fig\n\ndef plot_fisher_grid_aligned_new(param_names, param_fiducial, fisher_matrix_6x6, ax=None):\n    n_params = len(param_names)\n    \n    if ax is None:\n        ax = plt.gca()  # If no axis is provided, use the current axis\n    \n    fig, axes = plt.subplots(nrows=n_params, ncols=n_params, figsize=(12, 10), sharex='col', sharey='row')\n    fig.subplots_adjust(top=0.9, right=0.9, left=0.0, bottom=0.0, hspace=0.01, wspace=0.01)\n\n    neg_fmat = -fisher_matrix_6x6\n\n    for i in range(n_params):\n        for j in range(n_params):\n            if i &gt; j:\n                param_duo = np.array([param_fiducial[j], param_fiducial[i]])\n                fish2x2 = np.array([[neg_fmat[j, j], neg_fmat[j, i]], [neg_fmat[i, j], neg_fmat[i, i]]])\n                \n                # Pass the `ax` to plot_contours\n                plot_contours(fish2x2, param_duo, nstd=1., ax=ax, alpha=0.4, fill=True, edgecolor='red', linewidth=2)\n\n                if j == 0:\n                    ax.set_ylabel(param_names[i], fontsize=12)\n                if i == n_params - 1:\n                    ax.set_xlabel(param_names[j], fontsize=12)\n            else:\n                axes[i, j].axis('off')\n\n    fig.tight_layout()\n    fig.show()",
    "crumbs": [
      "EarlyDarkEmu"
    ]
  },
  {
    "objectID": "emu.html",
    "href": "emu.html",
    "title": "emu",
    "section": "",
    "text": "source\n\nenablePrint\n\n enablePrint ()\n\n\nsource\n\n\nblockPrint\n\n blockPrint ()\n\n\nsource\n\n\nemulate\n\n emulate (sepia_model:sepia.SepiaModel.SepiaModel=None,\n          input_params:&lt;built-infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format\n\n\ninput_params\narray\nNone\nInput parameter array\n\n\nReturns\ntuple\n\n2 np.array of mean and (0.05,0.95) quantile in prediction\n\n\n\n\nsource\n\n\nload_model_multiple\n\n load_model_multiple (model_dir:str=None, p_train_all:&lt;built-\n                      infunctionarray&gt;=None, y_vals_all:&lt;built-\n                      infunctionarray&gt;=None, y_ind_all:&lt;built-\n                      infunctionarray&gt;=None, z_index_range:&lt;built-\n                      infunctionarray&gt;=None, sepia_model_i:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dir\nstr\nNone\nPickle directory path\n\n\np_train_all\narray\nNone\nParameter array\n\n\ny_vals_all\narray\nNone\nTarget y-values array\n\n\ny_ind_all\narray\nNone\nx-values\n\n\nz_index_range\narray\nNone\nSnapshot indices for training\n\n\nsepia_model_i\nstr\nNone\n\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nemu_redshift\n\n emu_redshift (input_params_and_redshift:&lt;built-infunctionarray&gt;=None,\n               sepia_model_list:list=None, z_all:&lt;built-\n               infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_params_and_redshift\narray\nNone\nInput parameters (along with redshift)\n\n\nsepia_model_list\nlist\nNone\n\n\n\nz_all\narray\nNone\nAll the trained models",
    "crumbs": [
      "emu"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "pca",
    "section": "",
    "text": "source\n\ndo_pca\n\n do_pca (sepia_data:sepia.SepiaData.SepiaData=None,\n         exp_variance:float=0.98, do_discrepancy:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\nexp_variance\nfloat\n0.98\nExplained variance\n\n\ndo_discrepancy\nbool\nFalse\nFor discrepancy modeling\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel",
    "crumbs": [
      "pca"
    ]
  },
  {
    "objectID": "mcmc.html",
    "href": "mcmc.html",
    "title": "mcmc",
    "section": "",
    "text": "source\n\nln_prior\n\n ln_prior (theta, params_list)\n\n\nsource\n\n\nln_like\n\n ln_like (theta, redshift, x_grid, sepia_model_list, z_all, x, y, yerr)\n\n\nsource\n\n\nln_prob\n\n ln_prob (theta, redshift, params_list, x_grid, sepia_model_list, z_all,\n          x, y, yerr)\n\n\nsource\n\n\nchain_init\n\n chain_init (params_list, ndim, nwalkers)\n\n\nsource\n\n\ndefine_sampler\n\n define_sampler (redshift, ndim, nwalkers, params_list, x_grid,\n                 sepia_model_list, z_all, x, y, yerr)\n\n\nsource\n\n\ndo_mcmc\n\n do_mcmc (sampler, pos, nrun, ndim, if_burn=False)\n\n\nsource\n\n\nmcmc_results\n\n mcmc_results (samples)",
    "crumbs": [
      "mcmc"
    ]
  },
  {
    "objectID": "gp.html",
    "href": "gp.html",
    "title": "gp",
    "section": "",
    "text": "source\n\ndo_gp_train\n\n do_gp_train (sepia_model:sepia.SepiaModel.SepiaModel=None,\n              model_file:str=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format, after PCA\n\n\nmodel_file\nstr\nNone\npickle file path\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel after GP\n\n\n\n\nsource\n\n\ngp_load\n\n gp_load (sepia_model:sepia.SepiaModel.SepiaModel=None, model_file:str='/h\n          ome/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/modelmult\n          ivariate_model')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format (Pre-PCA is fine? – CHECK)\n\n\nmodel_file\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/modelmultivariate_model\npickle file path\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel\n\n\n\n\nsource\n\n\ngp_load_all\n\n gp_load_all ()\n\n\nsource\n\n\ndo_gp_train_multiple\n\n do_gp_train_multiple (model_dir:str=None, p_train_all:&lt;built-\n                       infunctionarray&gt;=None, y_vals_all:&lt;built-\n                       infunctionarray&gt;=None, y_ind_all:&lt;built-\n                       infunctionarray&gt;=None, z_index_range:&lt;built-\n                       infunctionarray&gt;=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dir\nstr\nNone\nPickle directory path\n\n\np_train_all\narray\nNone\nParameter array\n\n\ny_vals_all\narray\nNone\nTarget y-values array\n\n\ny_ind_all\narray\nNone\nx-values\n\n\nz_index_range\narray\nNone\nSnapshot indices for training\n\n\nReturns\nNone",
    "crumbs": [
      "gp"
    ]
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "viz",
    "section": "",
    "text": "source\n\nplot_lines_with_param_color\n\n plot_lines_with_param_color (param_array:&lt;built-infunctionarray&gt;=None,\n                              x_array:&lt;built-infunctionarray&gt;=None,\n                              y_array_all:&lt;built-infunctionarray&gt;=None,\n                              title_str:str=None, xlabel_str:str=None,\n                              ylabel_str:str=None,\n                              param_name_str:str=None,\n                              ax:matplotlib.axes._axes.Axes=None,\n                              y_log_plot_scale:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparam_array\narray\nNone\nparameter array\n\n\nx_array\narray\nNone\nx-axis array\n\n\ny_array_all\narray\nNone\ny-axis array\n\n\ntitle_str\nstr\nNone\nTitle string\n\n\nxlabel_str\nstr\nNone\nx-label string\n\n\nylabel_str\nstr\nNone\ny-label string\n\n\nparam_name_str\nstr\nNone\nParameter string,\n\n\nax\nAxes\nNone\n\n\n\ny_log_plot_scale\nbool\nFalse\n\n\n\n\n\nsource\n\n\nplot_scatter_matrix\n\n plot_scatter_matrix (df:pandas.core.frame.DataFrame=None,\n                      colors:str=None)\n\n\nsource\n\n\nplot_train_diagnostics\n\n plot_train_diagnostics (sepia_model:sepia.SepiaModel.SepiaModel=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format, after PCA\n\n\nReturns\ntuple\n\nPair-plot and Trace-plot\n\n\n\n\nsource\n\n\nsensitivity_plot\n\n sensitivity_plot (k_all:&lt;built-infunctionarray&gt;=None, params_all:&lt;built-\n                   infunctionarray&gt;=None,\n                   sepia_model:sepia.SepiaModel.SepiaModel=None,\n                   emulator_function=None, param_name:tuple=None,\n                   xy_lims:&lt;built-infunctionarray&gt;=[1e-05, 200000.0, 1,\n                   10000.0], y_log_plot_scale:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk_all\narray\nNone\nall wavenumbers\n\n\nparams_all\narray\nNone\nall parameters\n\n\nsepia_model\nSepiaModel\nNone\nSEPIA emulator model\n\n\nemulator_function\nNoneType\nNone\nfunction which takes in sepia model and parameters\n\n\nparam_name\ntuple\nNone\nParameter name\n\n\nxy_lims\narray\n[1e-05, 200000.0, 1, 10000.0]\n\n\n\ny_log_plot_scale\nbool\nFalse\n\n\n\n\n\nsource\n\n\nvalidation_plot\n\n validation_plot (k_all:&lt;built-infunctionarray&gt;=None, target_vals:&lt;built-\n                  infunctionarray&gt;=None, pred_mean:&lt;built-\n                  infunctionarray&gt;=None, pred_std:&lt;built-\n                  infunctionarray&gt;=None, xy_lims:&lt;built-\n                  infunctionarray&gt;=[0.02, 10.0, 0.98, 1.3],\n                  y_log_plot_scale:bool=False)\n\n\nsource\n\n\nplot_mcmc\n\n plot_mcmc (samples:&lt;built-infunctionarray&gt;, params_list:list,\n            if_truth_know:bool=False)\n\n\nsource\n\n\nplot_error_heatmap\n\n plot_error_heatmap (errors:&lt;built-infunctionarray&gt;=None,\n                     param_names:list=None, param_range:tuple=None)\n\n\nsource\n\n\ngenerate_param_grid_with_fixed\n\n generate_param_grid_with_fixed (param_name:list=None,\n                                 param_indices:&lt;built-\n                                 infunctionarray&gt;=None,\n                                 fixed_params:&lt;built-\n                                 infunctionarray&gt;=None, param_min:&lt;built-\n                                 infunctionarray&gt;=None, param_max:&lt;built-\n                                 infunctionarray&gt;=None, steps:int=40)",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "load.html",
    "href": "load.html",
    "title": "load",
    "section": "",
    "text": "source\n\nload_npy_pk_k_z\n\n load_npy_pk_k_z (Pk_fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkE\n                  mu/EarlyDarkEmu/data/pkg_data/pk_all.npy', k_fileIn:str=\n                  '/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEm\n                  u/data/pkg_data/k_all.npy', z_fileIn:str='/home/runner/w\n                  ork/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/pkg_data\n                  /z_all.npy', pk_log_scale:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nPk_fileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/pkg_data/pk_all.npy\nInput file for Pk\n\n\nk_fileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/pkg_data/k_all.npy\nInput file for k\n\n\nz_fileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/pkg_data/z_all.npy\nInput file for z\n\n\npk_log_scale\nbool\nTrue\nlog10 scaling for P(k)\n\n\nReturns\ntuple\n\nThree n-D arrays for P(k), k and z\n\n\n\n\n# Pk_all_ravel = np.ravel(Pk_all)\n\n# Pk_all_reshaped = np.reshape(Pk_all_ravel, newshape=(-1, z_all.shape[0], k_all.shape[0]))\n\n# (Pk_all_reshaped == Pk_all).all()\n\n\nsource\n\n\nload_sdss\n\n load_sdss (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyD\n            arkEmu/data/obs_data/reid_DR7.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/reid_DR7.txt\nInput file\n\n\n\n\nsource\n\n\nload_wmap\n\n load_wmap (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyD\n            arkEmu/data/obs_data/wmap_act.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/wmap_act.txt\nInput file\n\n\n\n\nsource\n\n\nload_lya\n\n load_lya (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDa\n           rkEmu/data/obs_data/eBOSS_DR14_Ly-a_Forest.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/eBOSS_DR14_Ly-a_Forest.txt\nInput file\n\n\n\n\nsource\n\n\nload_planck_ee\n\n load_planck_ee (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/E\n                 arlyDarkEmu/data/obs_data/planck2018_ee.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/planck2018_ee.txt\nInput file\n\n\n\n\nsource\n\n\nload_planck_tt\n\n load_planck_tt (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/E\n                 arlyDarkEmu/data/obs_data/planck2018_tt.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/planck2018_tt.txt\nInput file\n\n\n\n\nsource\n\n\nload_planck\n\n load_planck (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/Earl\n              yDarkEmu/data/obs_data/planck2018.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/planck2018.txt\nInput file\n\n\n\n\nsource\n\n\nload_des\n\n load_des (fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDa\n           rkEmu/data/obs_data/des_y1_cosmic_shear.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/obs_data/des_y1_cosmic_shear.txt\nInput file\n\n\n\n\n# #| export\n\n# def load_boost_data(Bk_fileIn:str=LIBRARY_BK_FILE, # Input file for Boost\n#                         Zk_fileIn:str=LIBRARY_ZK_FILE, # Input file for redshift and wavenumbers\n#                         ) -&gt; tuple: # Boost, wavenumbers, redshifts \n#     Bk_all = np.load(Bk_fileIn)\n#     zk_all = np.loadtxt(Zk_fileIn)\n    \n#     z_all = zk_all[:, 0][np.isfinite(zk_all[:, 0])]\n#     k_all = zk_all[:, 1]\n    \n#     return Bk_all, k_all, z_all\n\n\nsource\n\n\nload_params\n\n load_params (p_fileIn:str='/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/Ea\n              rlyDarkEmu/data/pkg_data/params_latin.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np_fileIn\nstr\n/home/runner/work/EarlyDarkEmu/EarlyDarkEmu/EarlyDarkEmu/data/pkg_data/params_latin.txt\nInput file for parameters\n\n\nReturns\narray\n\nParameters\n\n\n\n\nsource\n\n\nsepia_data_format\n\n sepia_data_format (design:&lt;built-infunctionarray&gt;=None, y_vals:&lt;built-\n                    infunctionarray&gt;=None, y_ind:&lt;built-\n                    infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndesign\narray\nNone\nParams array of shape (num_simulation, num_params)\n\n\ny_vals\narray\nNone\nShape (num_simulation, num_y_values)\n\n\ny_ind\narray\nNone\nShape (num_y_values,)\n\n\nReturns\nSepiaData\n\nSepia data format",
    "crumbs": [
      "load"
    ]
  },
  {
    "objectID": "fisher.html",
    "href": "fisher.html",
    "title": "fisher",
    "section": "",
    "text": "deriv\n\n deriv (params_array, redshift, forward_func, perturb=1e-05)\n\n\n\n\nFisherMatrix\n\n FisherMatrix (xvals, yvals, y_errors, params)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nplot_contours\n\n plot_contours (hessian, pos, nstd=1.0, ax=None, **kwargs)\n\n\n\n\nplot_fisher_grid_aligned\n\n plot_fisher_grid_aligned (param_names, param_fiducial, fisher_matrix_6x6)\n\n\n\n\nplot_fisher_grid_multiple\n\n plot_fisher_grid_multiple (param_names, param_fiducial, fisher_matrices,\n                            matrix_labels)\n\n\n\n\ncompute_fisher_k\n\n compute_fisher_k (k_vals, param_fid, redshift, forward_func, pk_fid,\n                   pk_error_fid, perturb=1e-05)\n\n\n\n\ncomputs_fisher_mat\n\n computs_fisher_mat (params, redshift, forward_func, perturb=1e-05)",
    "crumbs": [
      "fisher"
    ]
  }
]